<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Neurosymbolic AI for EGI: Explainable, Grounded, and Instructable Generations">
    <title>Neurosymbolic AI for EGI</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            line-height: 1.6;
            color: #333;
            background-color: #e6f7ff;
        }

        .banner {
            background: #ff5722;
            color: white;
            text-align: center;
            padding: 1rem 0;
            font-weight: bold;
            font-size: 1.4rem;
            position: sticky;
            top: 0;
            z-index: 1000;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.2);
        }

        header {
            background: linear-gradient(to right, #673ab7, #3f51b5);
            color: white;
            padding: 2rem 0;
            text-align: center;
            margin-bottom: 1rem;
        }

        header h1 {
            margin: 0;
            font-size: 2.5rem;
        }

        nav {
            background: #1a237e;
            color: white;
            display: flex;
            justify-content: center;
            padding: 1rem 0;
            margin-bottom: 1rem;
        }

        nav a {
            color: #ffeb3b;
            text-decoration: none;
            margin: 0 2rem;
            font-size: 1.7rem;
        }

        nav a:hover {
            text-decoration: underline;
            color: #ffd600;
        }

        .container {
            padding: 2rem 10%;
        }

        section {
            margin-bottom: 3rem;
            padding: 2rem;
            background: #ffffff;
            border: 1px solid #ccc;
            border-radius: 8px;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
        }

        section h2 {
            color: #3f51b5;
            margin-bottom: 1rem;
            border-bottom: 3px solid #3f51b5;
            padding-bottom: 0.5rem;
            font-size: 2rem;
        }

        section h3 {
            color: #303f9f;
            margin-bottom: 1rem;
            font-size: 1.8rem;
        }
        section p {
        font-family: 'Roboto', Arial, sans-serif; /* Update font family if needed */
        font-size: 1.4rem; /* Adjust size (e.g., 0.9rem for smaller, 1.1rem for larger) */
        line-height: 1.8; /* Fine-tune line spacing */
        color: #444; /* Optional: Darker gray for better contrast */
    }
        section p, ul {
            margin-top: 1rem;
            font-size: 1.4rem;
        }

        ul {
            padding-left: 1.5rem;
        }

        ul li {
            margin-bottom: 0.5rem;
        }

        .abstract-content {
            display: flex;
            flex-wrap: wrap;
            align-items: flex-start;
        }

        .abstract-content img {
            float: right;
            width: 20%;
            max-width: 300px;
            height: auto;
            margin: 0 0 1rem 1rem;
            border: 2px solid #3f51b5;
            border-radius: 5px;
        }

        .abstract-content div {
            flex: 1;
            text-align: justify;
        }

        .team-member {
            display: flex;
            align-items: flex-start;
            gap: 1.5rem;
            margin-bottom: 2rem;
        }

        .team-member img {
            width: 150px;
            height: 150px;
            border-radius: 50%;
            border: 3px solid #3f51b5;
        }

        .team-member div {
            flex: 1;
        }

        footer {
            background: #1a237e;
            color: white;
            text-align: center;
            padding: 2rem 0;
            margin-top: 3rem;
        }

        hr {
            border: 0;
            height: 2px;
            background: #3f51b5;
            margin: 2rem 0;
        }


        .slides-container {
            display: flex;
            gap: 1rem;
            justify-content: center; /* Center slides horizontally */
            flex-wrap: wrap; /* Allow wrapping to the next line on smaller screens */
            padding: 1rem 0;
        }

        .slide {
            flex: 0 0 350px; /* Set a fixed width for each slide */
            height: 300px; /* Set a fixed height for each slide */
            text-align: center;
            background: #f9f9f9;
            border: 1px solid #ddd;
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            padding: 1rem;
            display: flex;
            flex-direction: column; /* Align content vertically */
            justify-content: space-between; /* Space out elements inside */
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }

        .slide img {
            width: 100%; /* Ensure the image fills the width */
            height: auto; /* Maintain aspect ratio */
            max-height: 150px; /* Limit image height to fit within the slide */
            border-radius: 5px;
        }

        .slide h3 {
            font-size: 1.4rem;
            margin: 0.5rem 0;
            color: #333;
        }

        .slide a {
            text-decoration: none;
            color: #1a237e;
            font-weight: bold;
        }
        .linkedin-logo {
            width: 24px;
            height: 24px;
            margin-left: 10px;
            vertical-align: middle;
            border-radius: 0; /* Ensures the logo is not in a circle */
        }
    </style>
</head>

<body>
<!--     <div class="banner">
        ⚠️ This page is currently under reconstruction and is a work in progress. Content may be incomplete or subject to change. ⚠️
    </div> -->

    <header>
        <div style="display: flex; justify-content: space-between; align-items: center; padding: 0 20px;">
            <!-- Left side logos -->
            <div style="flex: 1; display: flex; justify-content: center; align-items: center; gap: 15px;">
                <img src="logos/UMBC.png" alt="Logo 1" style="height: 50px;">
                <img src="logos/KAI2.png" alt="Logo 2" style="height: 50px;">
                <img src="logos/AIUSC.png" alt="Logo 3" style="height: 50px;">
            </div>
    
            <!-- Main title -->
            <div style="text-align: center;">
                <h1>Tutorial: Neurosymbolic AI for EGI</h1>
                <p>Explainable, Grounded, and Instructable Generations</p>
            </div>
    
            <!-- Right side logos -->
            <div style="flex: 1; display: flex; justify-content: center; align-items: center; gap: 15px;">
                <img src="logos/AWS.png" alt="Logo 4" style="height: 50px;">
                <img src="logos/IFH.png" alt="Logo 5" style="height: 50px;">
                <img src="logos/umbcAI.png" alt="Logo 6" style="height: 50px;">
            </div>
        </div>
    </header>

    <nav>
        <a href="#abstract">Abstract</a>
        <a href="#schedule">Schedule</a>
        <a href="#team">Organizer Bios</a>
        <a href="#audience">Audience</a>
        <a href="#slides">Slides</a>
        <a href="#references">References</a>
    </nav>

    <div class="container">
        <section id="abstract">
            <h2>Tutorial Abstract</h2>
            <div class="abstract-content">
                <div>
                    <img src="images/abstract.png" alt="Abstract Image">
                    <p>Large Language Models (LLMs) are transforming Natural Language Processing tasks across multiple domains. Despite their capabilities, their real-world adoption is often limited by issues like the lack of transparency, inadequate understanding of domain protocols, and subpar precision. This tutorial introduces the concept of Neurosymbolic AI, which combines symbolic knowledge structures with statistical learning techniques to build more robust, explainable, and instructable LLMs. This tutorial aims to empower participants to deeply understand Neurosymbolic AI applied to Large Language Models (LLMs), addressing key challenges like explainability, grounding, and instructability (EGI).</p>
                    <p><strong>Specific learning outcomes:</strong></p>
                    <ul>
                        <li><strong>Understand limitations:</strong> Grasp the challenges of traditional black-box LLMs and the importance of EGI.</li>
                        <li><strong>Design LLMs:</strong> Learn to create models that integrate process knowledge for better instructability.</li>
                        <li><strong>Enhance grounding:</strong> Develop skills to strengthen LLMs in healthcare applications through reliable explanations.</li>
                        <li><strong>Personalize responses:</strong> Explore methods for tailoring LLM outputs using domain-specific knowledge.</li>
                        <li><strong>Ensure accountability:</strong> Assess outputs with a focus on provenance, reasoning, and transparency.</li>
                    </ul>
                </div>
            </div>
        </section>

        <hr>

        <section id="schedule">
            <h2>Schedule</h2>
            <p><strong>9:00 AM - 9:30 AM:</strong> Introduction to Neurosymbolic AI and Knowledge-infused Learning.</p>
            <p><strong>9:30 AM - 10:15 AM:</strong> Vector Symbolic Architectures by Edward Raff.</p>
            <p><strong>10:15 AM - 10:45 AM:</strong> Knowledge-infused Learning for Explainability and Instructibility.</p>
            <p><strong>10:45 AM - 11:30 AM:</strong> Grounding Blackbox Language Models with RAG.</p>
            <p><strong>11:30 AM - 12:00 PM:</strong> Building Explainable and Personalized Conversational Agents.</p>
            <p><strong>12:00 AM - 12:30 PM:</strong> OpenCHA: Building Explainable and Personalized Conversational.</p>
        </section>

        <hr>

        <section id="team">
            <h2>Tutorial Organizers/Presenters</h2>
        
            <div class="team-member">
                <img src="members/Deepa.png" alt="Deepa">
                <div>
                    <h3>
                        Deepa Tilwani
                        <a href="https://www.linkedin.com/in/deepa-tilwani-b758551a0/" target="_blank" style="color: #0077b5; font-weight: bold; font-size: 0.9rem; margin-left: 10px; text-decoration: none;">
                            LinkedIn
                        </a>
                    </h3>
                    <p>Deepa Tilwani is a PhD candidate in Computer Science at the Artificial Intelligence Institute, University of South Carolina, where she specializes in advancing Large Language Models, Artificial Intelligence, and Neurosymbolic AI. Her research lies at the intersection of AI and neuroscience, leveraging cutting-edge natural language processing, signal processing, and biosignal analysis to develop transformative solutions for complex neuroscience challenges. Recognized with prestigious awards and featured in leading publications, her work reflects significant impact and innovation in health care. Driven by a passion for discovery, she actively seeks opportunities to collaborate and contribute to groundbreaking advancements in this rapidly evolving field.</p>
                </div>
            </div>
        
            <div class="team-member">
                <img src="members/Ali.png" alt="Ali">
                <div>
                    <h3>
                        Ali (SeyedAli) Mohammadi
                        <a href="https://www.linkedin.com/in/ali-mohammadi-b0aab072/" target="_blank" style="color: #0077b5; font-weight: bold; font-size: 0.9rem; margin-left: 10px; text-decoration: none;">
                            LinkedIn
                        </a>
                    </h3>
                    <p>Ali Mohammadi is currently pursuing a Ph.D. at the University of Maryland, Baltimore County (UMBC), focusing on Safety-enabled Learning, Explainable Artificial Intelligence, Natural Language Processing, and Knowledge Graphs. He is actively engaged in research at the Knowledge-Inference and Knowledge-infused AI Inference Lab, under the guidance of Dr. Manas Gaur and Dr. Frank Ferraro. Prior to his doctoral studies, Ali served as a lecturer, imparting knowledge in courses such as Image Processing, Artificial Intelligence, and Data Structures. He holds a master's in Artificial Intelligence, further enriching his academic journey.</p>
                </div>
            </div>
        
            <div class="team-member">
                <img src="members/Edward.png" alt="Edward">
                <div>
                    <h3>
                        Edward Raff
                        <a href="https://www.linkedin.com/in/edward-raff-09992040/" target="_blank" style="color: #0077b5; font-weight: bold; font-size: 0.9rem; margin-left: 10px; text-decoration: none;">
                            LinkedIn
                        </a>
                    </h3>
                    <p>Edward Raff is the Director of Emerging AI at Booz Allen Hamilton and a visiting professor at the University of Maryland, Baltimore County. Dr. Raff’s research toward solving client problems covers topics in Cyber Security, Reproducibility, Adversarial Machine Learning, High-Performance Computing, and Neuro-Symbolic methods. As such, he has been elected as a senior member of AAAI and the IEEE, published two books, 130+ papers, 6 best-paper awards, and 10+ patents. He has co-chaired the Conference on Applied Machine Learning for Information Security (CAMLIS) three times and co-chaired the AAAI workshop on Cyber Security (AICS) three times, was reproducibility chair of AAAI and a senior program member of multiple AI/ML conferences. </p>
                </div>
            </div>
        
            <div class="team-member">
                <img src="members/Iman.png" alt="Iman">
                <div>
                    <h3>
                        Iman Azimi
                        <a href="https://www.linkedin.com/in/imanazimi/" target="_blank" style="color: #0077b5; font-weight: bold; font-size: 0.9rem; margin-left: 10px; text-decoration: none;">
                            LinkedIn
                        </a>
                    </h3>
                    <p>Iman Azimi is the assistant director at the UC Irvine - Institute for Future Health and an adjunct professor at the University of Turku. Iman holds a Ph.D. in Information and Communication Technology from the University of Turku and an MSc in Artificial Intelligence and Robotics from Sapienza University of Rome. His research spans machine learning, generative AI, and health data analysis. He works on multiple digital health projects, focusing on conversational health agents, mental health, maternal health and nutrition monitoring, and personalized data analytics utilizing AI techniques on diverse data sources, such as wearable devices.</p>
                </div>
            </div>
        
            <div class="team-member">
                <img src="members/Aman.png" alt="Aman">
                <div>
                    <h3>
                        Aman Chadha
                        <a href="https://www.linkedin.com/in/amanc/" target="_blank" style="color: #0077b5; font-weight: bold; font-size: 0.9rem; margin-left: 10px; text-decoration: none;">
                            LinkedIn
                        </a>
                    </h3>
                    <p>Aman Chadha leads a team of Generative AI Scientists and Managers at AWS. Previously, he spearheaded Speaker Understanding & Personalization efforts at Amazon Alexa AI. Prior to Amazon, Aman was a key contributor to the Machine Intelligence Neural Design (MIND) team at Apple, where he trained on-device multimodal AI models for applications spanning Natural Language Processing, Computer Vision, and Speech Recognition. As one of the architects behind Apple's M1 chip, he developed machine learning models to predict the performance of future Macs years in advance. Before Apple, Aman honed his expertise in ML accelerators and GPUs during his tenure at Qualcomm and Nvidia. Aman specialized in Multimodal AI at Stanford University. He holds a Master’s in Computer Engineering from the University of Wisconsin-Madison, where he received the Outstanding Graduate Student Award, and a Bachelor’s in Electronics and Telecommunication Engineering with distinction from the University of Mumbai. He has published in leading conferences, such as ACL, EMNLP (Outstanding Paper Award '23), AAAI, EACL, ECIR, ECML, WSDM, WACV, ICASSP, etc. His work has been featured in outlets like The Washington Post, New Scientist, Analytics India Magazine, etc. </p>
                </div>
            </div>
        
            <div class="team-member">
                <img src="members/Manas.png" alt="Manas">
                <div>
                    <h3>
                        Manas Gaur
                        <a href="https://www.linkedin.com/in/manasgaur/" target="_blank" style="color: #0077b5; font-weight: bold; font-size: 0.9rem; margin-left: 10px; text-decoration: none;">
                            LinkedIn
                        </a>
                    </h3>
                    <p>Manas Gaur is an assistant professor in the Department of Computer Science and Electrical Engineering at the University of Maryland Baltimore County. He leads the Knowledge-infused AI Inference Lab focusing on NeuroSymbolic AI, Explainable AI,  Safe AI, Knowledge-infused Learning, Large Language Models, and Knowledge Graphs, with applications to mental health, cybersecurity, crisis informatics, and conversational systems. Previously he was a senior research scientist at Samsung Research America and a visiting researcher at Alan Turing Institute UK. He was an AI for Social Good Fellow at Dataminr Inc. and Eric Wendy Schmidt Data Science for Social Good Fellow at the University of Chicago. His research has received the best paper award in IEEE Internet Computing, IEEE Intelligent Systems and an honorable mention award at ACM CoDS COMAD.  He was selected for AAAI New Faculty Highlights, and USC was awarded the Eminent Doctoral Profile award. He has been a guest editor on NeuroSymbolic AI and Large Language Models in IEEE Internet Computing and ACM Transactions on Computing for Health. He holds senior PC member or area chair for WWW, KDD, CIKM, AAAI, and ACL.  He is Co-Chair of the International Semantic Web Conference. He has organized the first Tutorial on Knowledge-infused Learning (AAAI, ACM Hypertext, and Social Computing), Explainable AI using Knowledge Graphs (AI-ML Systems, KDD).</p>
                </div>
            </div>
        
        </section>
        
        
        <hr>

        <section id="audience">
            <h2>Tutorial Audience</h2>
            <p>This tutorial is designed to appeal to a broad audience, from graduate students seeking a foundational understanding of Neurosymbolic AI to industry professionals exploring its practical applications. Graduate students will benefit from a comprehensive introduction to the field, while faculty members can delve into cutting-edge research on knowledge graph-driven generative AI, particularly in healthcare. Industry researchers will gain valuable insights into grounding, instructability, and explainability in agents, and a hands-on demonstration will showcase how to integrate Neurosymbolic AI into real-world scenarios.</p>

            <h3>Prerequisites</h3>
            <p>While this tutorial is designed to be accessible, a foundational understanding of the following topics is recommended to maximize comprehension and engagement:</p>
            <ul>
                <li><strong>Probability and Statistics:</strong> Familiarity with basic probability concepts and statistical analysis methods.</li>
                <li><strong>Artificial Intelligence (AI):</strong> A working knowledge of AI principles, especially those related to language models and neural networks.</li>
                <li><strong>Knowledge Graphs:</strong> Understanding the structure and application of knowledge graphs in AI systems.</li>
                <li><strong>Machine Learning:</strong> Experience with machine learning concepts, including supervised and unsupervised learning techniques.</li>
                <li><strong>Data Mining:</strong> Basic insights into extracting patterns and knowledge from large datasets.</li>
            </ul>
            <p>For those new to these areas, the tutorial will include a foundational overview and focus on providing intuitive explanations of complex concepts. Advanced topics will be presented with an emphasis on accessibility, ensuring participants of varied expertise can follow along and benefit from the session.</p>


            <h3>Takeaways</h3>
            <p>
                While grounding in AI is often associated with multimodality, it encompasses a broader concept 
                <a href="#ref-1">[1]</a>. Grounding refers to ensuring that an AI system's understanding is firmly rooted in domain-specific knowledge, guidelines, and expertise 
                <a href="#ref-2">[2]</a>. This is crucial for preventing superficial responses that often plague current LLMs 
                <a href="#ref-3">[3]</a>. By delving into the techniques and strategies for achieving stronger grounding, participants will learn how to: 
                <ul>
                    <li>
                        Anchor AI models in factual information through guidelines, graphs, and domain knowledge bases 
                        <a href="#ref-4">[4]</a>
                    </li>
                    <li>
                        Align AI responses with specific contexts and not provide an inconsistent and biased response
                    </li>
                    <li>
                        Mitigate the risk of AI generating inaccurate or misleading information through attribution
                    </li>
                </ul>
            </p>

            
        </section>

        <hr>

        <section id="slides">
            <h2>Slides</h2>
            <div class="slides-container">
                <div class="slide">
                    <a href="https://docs.google.com/presentation/d/1u1T7RWLu6gKG_nibajyfZm3z7njx5LJR/edit?usp=sharing&ouid=115373182588084065297&rtpof=true&sd=true" target="_blank">
                        <img src="images/slide1.png" alt="Slide 1">
                        <h3>Robustness in Large Language Models with NeuroSymbolic AI</h3>
                    </a>
                </div>
                <div class="slide">
                    <a href="https://docs.google.com/presentation/d/1ktpM7RuiS_k6p7Ymn_LrONW0FjJrXkT_/edit?usp=sharing&ouid=115373182588084065297&rtpof=true&sd=true" target="_blank">
                        <img src="images/slide2.png" alt="Slide 2">
                        <h3>Retrieval Augmented Generation Model For Knowledge-Intensive Learning NLP Task </h3>
                    </a>
                </div>
                <div class="slide">
                    <a href="https://docs.google.com/presentation/d/1v5Xfiu5ZGKuffgvg0QoWm1XkYYxKLqoF/edit?usp=sharing&ouid=115373182588084065297&rtpof=true&sd=true" target="_blank">
                        <img src="images/slide3.png" alt="Slide 3">
                        <h3>Trustworthy Generative AI: 
                            The Hybridization of Large Language Models with NeuroSymbolic AI
                            </h3>
                    </a>
                </div>
            </div>
        </section>
        
        

        <hr>

        <section id="references">
            <h2>References</h2>
            <ol>
                <li id="ref-1">Antonio Nucci. LLM Grounding: Techniques to Amplify AI Model Accuracy --- aisera.com. aisera.com/blog/llm-grounding, [Accessed 17-01-2025]</li>
                <li id="ref-2">Bajaj G, Shalin VL, Parthasarathy S, Sheth A. Grounding From an AI and Cognitive Science Lens. IEEE Intelligent Systems. 2024 Apr 30;39(2):66-71.</li>
                <li id="ref-3">Biderman S, Prashanth U, Sutawika L, Schoelkopf H, Anthony Q, Purohit S, Raff E. Emergent and predictable memorization in large language models. Advances in Neural Information Processing Systems. 2024 Feb 13;36.</li>
                <li id="ref-4">Grounding in Large Language Models (LLMs) and AI | Generative AI Wiki --- attri.ai. https://attri.ai/generative-ai-wiki/grounding-in-large-language-models-llms-and-ai, [Accessed 17-01-2025]</li>

            </ol>
        </section>
    </div>

    <footer>
        <p>&copy; 2025 Neurosymbolic AI Tutorial. All Rights Reserved.</p>
    </footer>
</body>
</html>
